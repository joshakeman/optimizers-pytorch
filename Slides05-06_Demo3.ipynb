{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch walkthrough generated by modifying & combining several tutorials:\n",
    "  - https://morvanzhou.github.io/tutorials/\n",
    "\n",
    "This notebeook will walk through:\n",
    "  * Learning rate adaptation (Different optimizers)\n",
    "  * Batch normalization\n",
    "  * Dropout\n",
    "\n",
    "Dependencies (tested on):\n",
    "* torch: 0.3.0\n",
    "* torchvision\n",
    "* matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake dataset\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\n",
    "y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n",
    "\n",
    "# plot dataset\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put dataset into torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(1, 20)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(20, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_SGD         = Net()\n",
    "net_Momentum    = Net()\n",
    "net_RMSprop     = Net()\n",
    "net_Adam        = Net()\n",
    "nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\n",
    "opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n",
    "opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\n",
    "opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "losses_his = [[], [], [], []]   # record loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch)\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):          # for each training step\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "\n",
    "        for net, opt, l_his in zip(nets, optimizers, losses_his):\n",
    "            output = net(b_x)              # get output for every net\n",
    "            loss = loss_func(output, b_y)  # compute loss for every net\n",
    "            opt.zero_grad()                # clear gradients for next train\n",
    "            loss.backward()                # backpropagation, compute gradients\n",
    "            opt.step()                     # apply gradients\n",
    "            l_his.append(loss.data[0])     # loss recoder\n",
    "\n",
    "labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n",
    "for i, l_his in enumerate(losses_his):\n",
    "    plt.plot(l_his, label=labels[i])\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim((0, 0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "N_HIDDEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "y = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# test data\n",
    "test_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "test_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_x, test_y = Variable(test_x, volatile=True), Variable(test_y, volatile=True)\n",
    "\n",
    "# show data\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net_overfitting)  # net architecture\n",
    "print(net_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    pred_ofit = net_overfitting(x)\n",
    "    pred_drop = net_dropped(x)\n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    loss_ofit.backward()\n",
    "    loss_drop.backward()\n",
    "    optimizer_ofit.step()\n",
    "    optimizer_drop.step()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # change to eval mode in order to fix drop out effect\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()  # parameters for dropout differ from train mode\n",
    "\n",
    "        # plotting\n",
    "        plt.cla()\n",
    "        test_pred_ofit = net_overfitting(test_x)\n",
    "        test_pred_drop = net_dropped(test_x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        plt.text(0, -1.2, 'overfitting loss=%.4f' % loss_func(test_pred_ofit, test_y).data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(0, -1.5, 'dropout loss=%.4f' % loss_func(test_pred_drop, test_y).data[0], fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "\n",
    "        # change back to train mode\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "N_SAMPLES = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 12\n",
    "LR = 0.03\n",
    "N_HIDDEN = 8\n",
    "ACTIVATION = F.tanh\n",
    "B_INIT = -0.2   # use a bad bias constant initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "x = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 2, x.shape)\n",
    "y = np.square(x) - 5 + noise\n",
    "\n",
    "# test data\n",
    "test_x = np.linspace(-7, 10, 200)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 2, test_x.shape)\n",
    "test_y = np.square(test_x) - 5 + noise\n",
    "\n",
    "train_x, train_y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "test_x = Variable(torch.from_numpy(test_x).float(), volatile=True)  # not for computing gradients\n",
    "test_y = Variable(torch.from_numpy(test_y).float(), volatile=True)\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n",
    "\n",
    "# show data\n",
    "plt.scatter(train_x.numpy(), train_y.numpy(), c='#FF9359', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batch_normalization=False):\n",
    "        super(Net, self).__init__()\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.bn_input = nn.BatchNorm1d(1, momentum=0.5)   # for input data\n",
    "\n",
    "        for i in range(N_HIDDEN):               # build hidden layers and BN layers\n",
    "            input_size = 1 if i == 0 else 10\n",
    "            fc = nn.Linear(input_size, 10)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(10, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(10, 1)         # output layer\n",
    "        self._set_init(self.predict)            # parameters initialization\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        init.normal(layer.weight, mean=0., std=.1)\n",
    "        init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(N_HIDDEN):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out, layer_input, pre_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [Net(batch_normalization=False), Net(batch_normalization=True)]\n",
    "\n",
    "print(nets[0])    # print net architecture\n",
    "print(nets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = [torch.optim.Adam(net.parameters(), lr=LR) for net in nets]\n",
    "\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(4, N_HIDDEN+1, figsize=(10, 5))\n",
    "plt.ion()   # something about plotting\n",
    "def plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n",
    "    for i, (ax_pa, ax_pa_bn, ax,  ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n",
    "        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n",
    "        if i == 0: p_range = (-7, 10);the_range = (-7, 10)\n",
    "        else:p_range = (-4, 4);the_range = (-1, 1)\n",
    "        ax_pa.set_title('L' + str(i))\n",
    "        ax_pa.hist(pre_ac[i].data.numpy().ravel(), bins=10, range=p_range, color='#FF9359', alpha=0.5);ax_pa_bn.hist(pre_ac_bn[i].data.numpy().ravel(), bins=10, range=p_range, color='#74BCFF', alpha=0.5)\n",
    "        ax.hist(l_in[i].data.numpy().ravel(), bins=10, range=the_range, color='#FF9359');ax_bn.hist(l_in_bn[i].data.numpy().ravel(), bins=10, range=the_range, color='#74BCFF')\n",
    "        for a in [ax_pa, ax, ax_pa_bn, ax_bn]: a.set_yticks(());a.set_xticks(())\n",
    "        ax_pa_bn.set_xticks(p_range);ax_bn.set_xticks(the_range)\n",
    "        axs[0, 0].set_ylabel('PreAct');axs[1, 0].set_ylabel('BN PreAct');axs[2, 0].set_ylabel('Act');axs[3, 0].set_ylabel('BN Act')\n",
    "    plt.pause(0.01)\n",
    "\n",
    "def unpack_two(arg1, arg2, *rest):\n",
    "    return arg1, arg2, rest\n",
    "\n",
    "# training\n",
    "losses = [[], []]  # recode loss for two networks\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch)\n",
    "    layer_inputs, pre_acts = [], []\n",
    "    for net, l in zip(nets, losses):\n",
    "        net.eval()              # set eval mode to fix moving_mean and moving_var\n",
    "        pred, layer_input, pre_act = net(test_x)\n",
    "        l.append(loss_func(pred, test_y).data[0])\n",
    "        layer_inputs.append(layer_input)\n",
    "        pre_acts.append(pre_act)\n",
    "        net.train()             # free moving_mean and moving_var\n",
    "    plot_histogram(layer_inputs[0], layer_inputs[1], pre_acts[0], pre_acts[1])     # plot histogram\n",
    "\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "        b_x, b_y = Variable(b_x), Variable(b_y)\n",
    "        for net, opt in zip(nets, opts):     # train for each network\n",
    "            pred, _, _ = net(b_x)\n",
    "            loss = loss_func(pred, b_y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()    # it will also learns the parameters in Batch Normalization\n",
    "            \n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plt.figure(2)\n",
    "plt.plot(losses[0], c='#FF9359', lw=3, label='Original')\n",
    "plt.plot(losses[1], c='#74BCFF', lw=3, label='Batch Normalization')\n",
    "plt.xlabel('step');plt.ylabel('test loss');plt.ylim((0, 2000));plt.legend(loc='best')\n",
    "\n",
    "# evaluation\n",
    "# set net to eval mode to freeze the parameters in batch normalization layers\n",
    "[net.eval() for net in nets]    # set eval mode to fix moving_mean and moving_var\n",
    "preds = [net(test_x)[0] for net in nets]\n",
    "plt.figure(3)\n",
    "plt.plot(test_x.data.numpy(), preds[0].data.numpy(), c='#FF9359', lw=4, label='Original')\n",
    "plt.plot(test_x.data.numpy(), preds[1].data.numpy(), c='#74BCFF', lw=4, label='Batch Normalization')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='r', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
